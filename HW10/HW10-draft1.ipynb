{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#HW 10 CLUSTERING BUSINESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###cluster time trends in NYC businesses: \n",
    "###DATA: Census Business data:\n",
    "###download census data for businesses by ZIP code. the data is here\n",
    "http://www.census.gov/econ/cbp/download/\n",
    "\n",
    "###and it can be downloaded by hand. you can also download it with 3 terminal commands as follows: the data from 1993 through 2001 is different in the format of its path than the data after 2001 (that is why more than one for loop is needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ for ((y=93; y<=99; y+=1)); do wget ftp://ftp.census.gov/Econ2001_And_Earlier/CBP_CSV/zbp$y\\totals.zip; done\n",
    "\n",
    "$ for ((y=0; y<=9; y+=1)); do wget ftp://ftp.census.gov/econ200$y\\/CBP_CSV/zbp0$y\\totals.zip; done\n",
    "\n",
    "$ for ((y=10; y<=15; y+=1)); do wget ftp://ftp.census.gov/econ20$y\\/CBP_CSV/zbp$y\\totals.zip; done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#NYC zip codes shape file is here\n",
    "##http://data.nycprepared.org/dataset/nyc-zip-code-tabulation-areas/resource/0c0e14e9-78e1-404e-97b0-c2fabceb3981\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES: \n",
    "to read in a zip file without unzipping it you can use the pandas and zipfile packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import zipfiles #i am not giving it a name cause i intend to use it only once\n",
    "#zf = zipfile.ZipFile(fname)\n",
    "#df = pd.read_csv(zf.open(fname.replace('.zip','.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may need to clean your data: for some NYC zip codes there may be no info\n",
    "    \n",
    "sanity check: you should have 20 (Ntimestamps) datapoints per time series and about 250 zipcodes (Nzipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "IMPORTANT: we talked about the importance of \"whitening\" your data: dividing each feature by its standard deviation. \n",
    "Whitenings decorrelates the data: it makes the features independent so that the data covariance matrix is the identity matrix.\n",
    "Whitening your data in time series analysis is in most cases **wrong**: you are modifying your time behaviour. This is because of the strong correlation between features (two consecutive time stamps for the same observation, the same zip code here, are strongly correlated). Here instead you want to standardize your time series: subtract the mean and divide each time series (separately) by its standard deviation. As a sanity check (if you use skitlearn Kmeans or skitlearns kmeans2): you want your data array to be shaped Nzipcodes x Ntimestamps\n",
    "\n",
    "mydata.shape should be (Nzipcodes, Ntimestamps)\n",
    "\n",
    "mydata[i].std() shoould be 1 for all i in range(len(Nzipcodes))\n",
    "\n",
    "mydata[i].mean() should be ~0 for all i in range(len(Nzipcodes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TASKS:\n",
    "    \n",
    "    1. get and prep your data.\n",
    "    2. cluster the time series with K-means in **a few** clusters (as discussed there is no real good, sound way to decide what a good number is here. try a few options, keeping in mind a few is more than a couple, but i recommand you stay within the single digit numbers)\n",
    "    3. plot the cluster centers (if you used K means those are the means of the clusters). you can plot for example the cluster centers overlayed on each time series (using the alpha channel to control the opacity in the plot may be helpful here).\n",
    "    4. Use another clustering algorithm (of your choice)\n",
    "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n",
    "    6. Compare the results of the 2 algorithms\n",
    "    7. attempt an interpretation. this is dangerous ground: clustering is an exploratory tool so you do not want to jump to conclusions because you see some clusters! but seeing structure in your data can inform your next moves as an investigator. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['imread']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "__author__ = 'mo'\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import json\n",
    "\n",
    "import sklearn.cluster\n",
    "from sklearn import mixture\n",
    "from PIL import Image\n",
    "from scipy.misc import imread\n",
    "from scipy.cluster.vq import kmeans2\n",
    "import os\n",
    "\n",
    "kelly_colors_hex = [\n",
    "    '#FFB300', # Vivid Yellow\n",
    "    '#803E75', # Strong Purple\n",
    "    '#FF6800', # Vivid Orange\n",
    "    '#A6BDD7', # Very Light Blue\n",
    "    '#C10020', # Vivid Red\n",
    "    '#CEA262', # Grayish Yellow\n",
    "    '#817066', # Medium Gray\n",
    "    '#007D34', # Vivid Green\n",
    "    '#F6768E', # Strong Purplish Pink\n",
    "    '#00538A', # Strong Blue\n",
    "    '#FF7A5C', # Strong Yellowish Pink\n",
    "    '#53377A', # Strong Violet\n",
    "    '#FF8E00', # Vivid Orange Yellow\n",
    "    '#B32851', # Strong Purplish Red\n",
    "    '#F4C800', # Vivid Greenish Yellow\n",
    "    '#7F180D', # Strong Reddish Brown\n",
    "    '#93AA00', # Vivid Yellowish Green\n",
    "    '#593315', # Deep Yellowish Brown\n",
    "    '#F13A13', # Vivid Reddish Orange\n",
    "    '#232C16', # Dark Olive Green\n",
    "    ]\n",
    "\n",
    "#np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mariaortiz/PUI2015_mortiz/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print (os.environ['PUI2015'])\n",
    "#\"HOME\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-be02573e2dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#VERY unpythonic way of reading in data from csvs, and adding column for year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mzf94\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$PUI2015'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/HW10/zbp94totals.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mzf94\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1994'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mzf95\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$PUI2015'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/HW10/zbp95totals.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "#VERY unpythonic way of reading in data from csvs, and adding column for year\n",
    "zf94 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp94totals.csv')\n",
    "zf94['year'] = '1994'\n",
    "\n",
    "zf95 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp95totals.csv')\n",
    "zf95['year'] = '1995'\n",
    "\n",
    "zf96 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp96totals.csv')\n",
    "zf96['year'] = '1996'\n",
    "\n",
    "zf97 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp97totals.csv')\n",
    "zf97['year'] = '1997'\n",
    "\n",
    "zf98 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp98totals.csv')\n",
    "zf98['year'] = '1998'\n",
    "\n",
    "zf99 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp99totals.csv')\n",
    "zf99['year'] = '1999'\n",
    "\n",
    "zf00 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp00totals.csv')\n",
    "zf00['year'] = '2000'\n",
    "\n",
    "zf01 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp01totals.csv')\n",
    "zf01['year'] = '2001'\n",
    "\n",
    "zf02 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp02totals.csv')\n",
    "zf02['year'] = '2002'\n",
    "\n",
    "zf03 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp03totals.csv')\n",
    "zf03['year'] = '2003'\n",
    "\n",
    "zf04 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp04totals.csv')\n",
    "zf04['year'] = '2004'\n",
    "\n",
    "zf05 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp05totals.csv')\n",
    "zf05['year'] = '2005'\n",
    "\n",
    "zf06 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp06totals.csv')\n",
    "zf06['year'] = '2006'\n",
    "\n",
    "zf07 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp07totals.csv')\n",
    "zf07['year'] = '2007'\n",
    "\n",
    "zf08 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp08totals.csv')\n",
    "zf08['year'] = '2000'\n",
    "\n",
    "zf09 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp09totals.csv')\n",
    "zf09['year'] = '2009'\n",
    "\n",
    "zf10 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp10totals.csv')\n",
    "zf10['year'] = '2010'\n",
    "\n",
    "zf11 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp11totals.csv')\n",
    "zf11['year'] = '2011'\n",
    "\n",
    "zf12 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp12totals.csv')\n",
    "zf12['year'] = '2012'\n",
    "\n",
    "zf13 = pd.read_csv(os.getenv('$PUI2015')+'/HW10/zbp13totals.csv')\n",
    "zf13['year'] = '2013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sanity check and looking at data\n",
    "#zf03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zf94' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8f9edba660e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#creating small dataframes for each year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf94\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzf94\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf95\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzf95\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf96\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzf96\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf97\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzf97\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zf94' is not defined"
     ]
    }
   ],
   "source": [
    "#creating small dataframes for each year\n",
    "df94 = zf94[['zip', 'est', 'year', 'name']]\n",
    "df95 = zf95[['zip', 'est', 'year', 'name']]\n",
    "df96 = zf96[['zip', 'est', 'year', 'name']]\n",
    "df97 = zf97[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "#for some of these csvs, the headers change to caps, so again in very unpythonic way, changing the headers \n",
    "zf98['zip'] = zf98['ZIP']\n",
    "zf98['est'] = zf98['EST']\n",
    "zf98['name'] = zf98['NAME']\n",
    "df98 = zf98[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "zf99['zip'] = zf99['ZIP']\n",
    "zf99['est'] = zf99['EST']\n",
    "zf99['name'] = zf99['NAME']\n",
    "df99 = zf99[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "zf00['zip'] = zf00['ZIP']\n",
    "zf00['est'] = zf00['EST']\n",
    "zf00['name'] = zf00['NAME']\n",
    "df00 = zf00[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "zf01['zip'] = zf01['ZIP']\n",
    "zf01['est'] = zf01['EST']\n",
    "zf01['name'] = zf01['NAME']\n",
    "df01 = zf01[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "zf02['zip'] = zf02['ZIP']\n",
    "zf02['est'] = zf02['EST']\n",
    "zf02['name'] = zf02['NAME']\n",
    "df02 = zf02[['zip', 'est', 'year', 'name']]\n",
    "\n",
    "#lower case headers again\n",
    "df03 = zf03[['zip', 'est', 'year', 'name']]\n",
    "df04 = zf04[['zip', 'est', 'year', 'name']]\n",
    "df05 = zf05[['zip', 'est', 'year', 'name']]\n",
    "df06 = zf06[['zip', 'est', 'year', 'name']]\n",
    "df07 = zf07[['zip', 'est', 'year', 'name']]\n",
    "df08 = zf08[['zip', 'est', 'year', 'name']]\n",
    "df09 = zf09[['zip', 'est', 'year', 'name']]\n",
    "df10 = zf10[['zip', 'est', 'year', 'name']]\n",
    "df11 = zf11[['zip', 'est', 'year', 'name']]\n",
    "df12 = zf12[['zip', 'est', 'year', 'name']]\n",
    "df13 = zf13[['zip', 'est', 'year', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789999, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe with all years\n",
    "bigdata = pd.concat((df94, df95, df96, df97, df98, df99, df00, df01, df02, df03, df04, df05, df06, df07, df08, df09, df10, df11, df12, df13), axis = 0)\n",
    "bigdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading in shapefile\n",
    "shapeData = gp.read_file('/Users/mariaortiz/PUI2015_mortiz/HW10/nycpolygons.geojson')\n",
    "#making column to merge on Zip later on\n",
    "shapeData['ZIP'] = shapeData.postalCode.astype(int)\n",
    "#array of nyczips\n",
    "nyczips = shapeData.postalCode.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#making df for all data, by zip code and by year\n",
    "dfall = bigdata.loc[bigdata['zip'].isin(nyczips)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Awesome! \n",
    "\n",
    "####oh...wait a minute...\n",
    "Now we realized that the data needed to be put with 219 rows (219 zips) x 20 columns (20 years) with the value of est (# of establishments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaortiz/anaconda/lib/python2.7/site-packages/pandas/core/index.py:4072: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return np.sum(name == np.asarray(self.names)) > 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"19\" halign=\"left\">est</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>6999</td>\n",
       "      <td>7088</td>\n",
       "      <td>7075</td>\n",
       "      <td>7154</td>\n",
       "      <td>7254</td>\n",
       "      <td>7233</td>\n",
       "      <td>14789</td>\n",
       "      <td>7188</td>\n",
       "      <td>7141</td>\n",
       "      <td>7084</td>\n",
       "      <td>7370</td>\n",
       "      <td>7312</td>\n",
       "      <td>7415</td>\n",
       "      <td>7549</td>\n",
       "      <td>7305</td>\n",
       "      <td>7241</td>\n",
       "      <td>7227</td>\n",
       "      <td>7235</td>\n",
       "      <td>7273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>1784</td>\n",
       "      <td>1819</td>\n",
       "      <td>1919</td>\n",
       "      <td>2049</td>\n",
       "      <td>2003</td>\n",
       "      <td>2085</td>\n",
       "      <td>4968</td>\n",
       "      <td>2543</td>\n",
       "      <td>2269</td>\n",
       "      <td>2402</td>\n",
       "      <td>2551</td>\n",
       "      <td>2682</td>\n",
       "      <td>2686</td>\n",
       "      <td>2685</td>\n",
       "      <td>2775</td>\n",
       "      <td>2872</td>\n",
       "      <td>2912</td>\n",
       "      <td>2954</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>3256</td>\n",
       "      <td>3377</td>\n",
       "      <td>3417</td>\n",
       "      <td>3535</td>\n",
       "      <td>3649</td>\n",
       "      <td>3659</td>\n",
       "      <td>7754</td>\n",
       "      <td>3602</td>\n",
       "      <td>3616</td>\n",
       "      <td>3704</td>\n",
       "      <td>3776</td>\n",
       "      <td>3867</td>\n",
       "      <td>3909</td>\n",
       "      <td>4049</td>\n",
       "      <td>4099</td>\n",
       "      <td>4113</td>\n",
       "      <td>4159</td>\n",
       "      <td>4214</td>\n",
       "      <td>4277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>1115</td>\n",
       "      <td>1124</td>\n",
       "      <td>1133</td>\n",
       "      <td>1173</td>\n",
       "      <td>1176</td>\n",
       "      <td>1233</td>\n",
       "      <td>2670</td>\n",
       "      <td>1247</td>\n",
       "      <td>1253</td>\n",
       "      <td>1252</td>\n",
       "      <td>1283</td>\n",
       "      <td>1260</td>\n",
       "      <td>1311</td>\n",
       "      <td>1370</td>\n",
       "      <td>1379</td>\n",
       "      <td>1398</td>\n",
       "      <td>1404</td>\n",
       "      <td>1442</td>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>1076</td>\n",
       "      <td>1109</td>\n",
       "      <td>1216</td>\n",
       "      <td>1227</td>\n",
       "      <td>1270</td>\n",
       "      <td>1242</td>\n",
       "      <td>2578</td>\n",
       "      <td>1249</td>\n",
       "      <td>1244</td>\n",
       "      <td>1200</td>\n",
       "      <td>1226</td>\n",
       "      <td>1214</td>\n",
       "      <td>1259</td>\n",
       "      <td>1273</td>\n",
       "      <td>1274</td>\n",
       "      <td>1266</td>\n",
       "      <td>1301</td>\n",
       "      <td>1273</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        est                                                               \\\n",
       "year   1994  1995  1996  1997  1998  1999   2000  2001  2002  2003  2004   \n",
       "zip                                                                        \n",
       "10001  6999  7088  7075  7154  7254  7233  14789  7188  7141  7084  7370   \n",
       "10002  1784  1819  1919  2049  2003  2085   4968  2543  2269  2402  2551   \n",
       "10003  3256  3377  3417  3535  3649  3659   7754  3602  3616  3704  3776   \n",
       "10004  1115  1124  1133  1173  1176  1233   2670  1247  1253  1252  1283   \n",
       "10005  1076  1109  1216  1227  1270  1242   2578  1249  1244  1200  1226   \n",
       "\n",
       "                                                       \n",
       "year   2005  2006  2007  2009  2010  2011  2012  2013  \n",
       "zip                                                    \n",
       "10001  7312  7415  7549  7305  7241  7227  7235  7273  \n",
       "10002  2682  2686  2685  2775  2872  2912  2954  2988  \n",
       "10003  3867  3909  4049  4099  4113  4159  4214  4277  \n",
       "10004  1260  1311  1370  1379  1398  1404  1442  1479  \n",
       "10005  1214  1259  1273  1274  1266  1301  1273  1296  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonice = dfall[['zip', 'year', 'est']].groupby(['zip', 'year']).sum().unstack()\n",
    "sonice.reset_index().head()\n",
    "sonice.head()\n",
    "#sonice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6999.,  7088.,  7075., ...,  7227.,  7235.,  7273.],\n",
       "       [ 1784.,  1819.,  1919., ...,  2912.,  2954.,  2988.],\n",
       "       [ 3256.,  3377.,  3417., ...,  4159.,  4214.,  4277.],\n",
       "       ..., \n",
       "       [   95.,    90.,    88., ...,   131.,   129.,   127.],\n",
       "       [  294.,   285.,   287., ...,   326.,   326.,   327.],\n",
       "       [   33.,    32.,    34., ...,    46.,    49.,    40.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turning the df into an array\n",
    "final = sonice.values\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Standardizing time series: subtracting the mean and dividing each time series (separately) by its standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "yes = []\n",
    "for i in final:\n",
    "    rowstd = np.std(i)\n",
    "    rowmean = np.mean(i)\n",
    "    i = (i - rowmean)/rowstd\n",
    "    yes.append(i)\n",
    "\n",
    "yes2 = np.asarray(yes)\n",
    "yes2.shape\n",
    "\n",
    "#yes3 = pd.DataFrame(yes2)\n",
    "#yes3.head()\n",
    "#I only have 19 years...should be 20! will try to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##K means 2 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#my kernel dies every time...\n",
    "nk = 3\n",
    "#cluster_centroids, closest_centroids = kmeans2(yes2, nk, iter=1)\n",
    "cluster_centroids, closest_centroids = kmeans2(yes2, nk, iter=1, minit = 'points')\n",
    "#cluster_centroids, closest_centroids = kmeans((yes), nk, iter=20, thresh=1e-05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array=[1, 2, 3, 4, 4, 2]\n",
    "array2 = [2,3,6,3,6,7]\n",
    "test = concatenate([array, array2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting data for all years\n",
    "plt.figure(figsize=(10,4))\n",
    "#plt.xlim(1994,2012)\n",
    "\n",
    "for i in range(len(yes)):\n",
    "    pl.plot(yes2[i])\n",
    "    pl.xlabel(\"years\", fontsize=18)\n",
    "    pl.ylim(-3,3)\n",
    "    #pl.xlim(1994,2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cluster_centroids.shape)\n",
    "dates = list(range(1994,2013))\n",
    "cluster_centroidsDF = pd.DataFrame(cluster_centroids, columns = dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting centroids\n",
    "for i in range(0,3):\n",
    "    pl.plot(cluster_centroidsDF.loc[i,dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clusteryes = yes2.join(pd.Series(closest_centroids, name = 'Cluster Number'))\n",
    "cluster_yes = yes2.join(pd.Series(closest_centroids, name = 'Cluster Number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "badindex = []\n",
    "for i,est in enumerate(yes):\n",
    "    if np.isnan(est).any():\n",
    "        badindex.append(i)\n",
    "        \n",
    "yesnonan = np.delete(yes, badindex, 0)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=5.5, min_samples=5).fit(yesnonan)\n",
    "labels = (db.labels_).astype(int)\n",
    "#print labels\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "clusters = pd.Series([yesnonan[labels == i] for i in xrange(num_clusters)])\n",
    "print('Number of clusters: %d' % num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_myData2 = yesnonan.join(pd.Series(labels, name = 'DBSCAN Cluster Number'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nc = 5\n",
    "clustering = sklearn.cluster.KMeans(n_clusters=nc).fit(clean_est)\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting data for all years\n",
    "plt.figure(figsize=(10,4))\n",
    "#plt.xlim(1994,2012)\n",
    "\n",
    "for i in range(len(clean_est)):\n",
    "    pl.plot(clean_est[i])\n",
    "    pl.xlabel(\"years\", fontsize=18)\n",
    "    pl.ylim(-3,3)\n",
    "    #pl.xlim(1994,2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opening map data\n",
    "json_data=open('/Users/mariaortiz/PUI2015_mortiz/HW10/nyc-zip-code-tabulation-areas-polygons.geojson').read()\n",
    "\n",
    "data2 = json.loads(json_data)\n",
    "pl.plot(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mplleaflet\n",
    "#mplleaflet.display(crs=data.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Plotting on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
